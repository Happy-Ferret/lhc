Performance improvements over GHC:
  Node layout:
    * Tail pointer elimination. Tail pointers can always be compacted unless
      the tail object is shared.
      Likely to be a huge win.
    * Object inlining. Objects not in the tail position can be compacted iff
      their size is statically known. This basically means they cannot contain
      pointers to other heap objects. Inlineable objects include Char, Int,
      Integer, and so on. Objects not inlineable are Cons, Just, Branch, and
      so on.
      Likely to be a significant win.
  Mutator:
    * Aggressive unboxing. We have excellent GC support so we don't have to
      worry about unpacking heap objects into registers (or the stack). GHC
      is very limited in how it handles objects that may contain root pointers
      but we have no such limitations and can let the optimizer run freely.
      Small but robust win.
    * Interprocedural alias analysis. As a side-effect of the HPT analysis,
      we collect a lot of information about pointers which is of use to LLVM.
      We collect a complete aliasing profile (which pointers may or may not
      point to the same memory location), and we also identify which pointers
      only point to immutable memory (We're in Haskell afterall, there ought
      to be some immutability somewhere). LLVM loves this kind of information.
      Probably a large win in a very few situtations (loops and such),
      irrelevant in most others.
    * Interprocedural register allocation. 
      Likely to be a decent win.
  Garbage collector:
    * Eliminate two memory reads per live object. In GHC, info tables and block
      headers are accessed when evacuating/scavenging objects. We pack the
      needed information in the object tag.
      This should be a decent win but maybe not huge since the info tables and
      block headers often are in cache.
    * Static information is statically encoded. Information about which CAFs
      a certain object depends upon can be baked into the collector. No
      need to scan the info table to find static objects.
      This should be a very small win.
    * Cache friendly object layout. By packing objects together, we avoid
      reading objects smaller than a cache-line.
      Very likely to be a win.
    * Limited recursion in the evacuator. The evacuator statically knows when
      it is safe to evacuate the children of an object. This leads to better
      locality (see point above) and also means less work (fewer memory
      writes) for the scavenger.
    * Per-object stepping. GHC has a per-block target generation. This allows
      it to delay the promotion of objects until they've aged a bit more.
      We can have a per-object stepping counter which gives better control
      and allows objects to live longer in their generation.
      Might be a win, might not be a win.
  Laziness:
    * Update elimination. GHC pushes an update frame after each @eval. We do
      whole-program analysis to avoid updates when they are not needed.
      This should be a small but robust win.
    * Locality preserving @eval. In GHC, @eval touches the info table of the
      object to be evaluated. The cost of this is likely small since the table
      is probably cached. But not polluting the cache is always a good thing.
      Minor, unreliable win.
    * Locality preserving @update. Same as above except @update touches the
      block table. At least one block table is always touched. In some cases,
      two block tables are touched.
      Minor, unreliable win.
  Threading:
    * Linked list stack for green threads. Keeping a real stack for each thread
      is too expensive and it is often not more efficient than a linked list.
      This should be a win in some situations, a loss in others.
    * Single threaded mutator. All concurrency is shared-nothing.
  

Interesting avenues to explore:
  Immix.
  Supercompilation.
  Eager evaluation.

Profiling, debugging and program monitoring:
  Output dianostics continuously via nanomsg.
  Memory snapshot after each GC:
    Per generations:
      Per object type:
        Number of live
        Total size used
      Size of live objects
      Total size of generation
      Number of pointers to the previous generation
  Object duration is GC interval * (stepping+0.5)
  External tools can then listen to the snapshots and generate a graph of
  memory usage over time.

Stack frames and regions:
  In the presence of IO, we do not know the life span of stack frames.
  But in code that doesn't use IO (particularly in strict code), we
  know the exact lifespan of each stack frame. Could we use regions
  to avoid allocating the stack frames on the heap? We would require
  that there are no pointers from the heap to any region.

  In the absense of suspensions, the stack has the interesting property
  that it does not need to be part of the heap. It contains heap pointers
  but the heap will never point to it. This changes, of course, when we
  suspend computation.

  Can light-weight regions be implemented with llvm.stacksave and
  llvm.stackrestore? The regions couldn't overlap, of course, but maybe
  that is not necessary when we only use them for pushing frames to the
  stack. How to deal with exceptions? Hm, we can restore the stack when
  unwinding the stack.

Optimizations possible in bedrock which are not possible in LLVM:
  LLVM is doing a great job at optimizing our programs. We should focus
  on implementing those optimizations which cannot be performed on
  LLVM code.

  1. Hoisting code out of suspended functions. LLVM doesn't know that
     a node with the same name of a function represents the suspended
     form of said function.
     This optimization includes arity raising.
  2. Removing dead stores. We know that the heap is never walked and
     that objects are only accessed by their reference. If this reference
     is dead, the store can be omitted. I wish this information could be
     conveyed to LLVM but I don't see how.
  3. Node inlining and specialization. 

Using libuv
  We want one event loop per core with N heaps per event loop. Finding
  root elements for a given heap will be difficult.
  We basically want StablePtr's. How efficiently can we support them? How
  are they implemented in GHC right now?
  How about 'IORef (Map StablePtr Any)'? How will it interact with the HPT
  analysis? Use a judy array instead?

Shared nothing concurrency.
  Allocate a fixed number of isolated heaps per core.
  Round-robin allocation of new actors in each core/heap pair.
  Communication primitives:
    IORef: Simple mutable referrences. no locks, no sharing, no magic.
    MVar: Like IORefs but can block on reads and writes. The scheduling is part
          of the user-level implementation, not the RTS. No locks, no sharing.
    SVar: Like MVar but can be used to share values between actors. Has higher
          overhead than an MVar.
    SChan: Like Chan but can be shared.

Holes
  How to compile this: let ones = 1 : ones

  ptr <- store (Cons 1 hole)
  update ptr[2] ptr

  ptr <- alloc
  update ptr (Cons 1 ptr)

  Implicit fix-point? Lowered to alloc+update later.
  ptr <- store (Cons 1 ptr)

Garbage collection of CAFs.
  Trivial. We statically know which CAFs are referrable by all
  suspended functions.

Constant objects.
  GHC statically allocates a bag of small objects (ints and chars of small
  value) because they're very common. Would it be worth it to generalize this
  technique?

Garbage collection:
  Copying collector:
    Evacuate roots.
    Scavenge to-space.

  Mark/Sweep:
    How is marking done without blowing the stack? It isn't. :(
    The mark queue could be allocated on the heap. The smallest chain link
    take up 2 words. Since the mark queue needs roughly 1 word per chain link,
    we'd need to reserve 1/3 of the heap for marking.
    By marking tail-recursively we can queue one child fewer. Thus making the
    smallest chain link take up 3 words. Reserved space would then be 1/4 of
    heap. If we use another 1/4 to allow for compaction then only half the
    heap is available to the mutator before a GC is triggered.

  Trivial collectors:
    NoGC, fixed. Allocate a single chunk, never grow it.
    NoGC, growing. Allocate as needed, never collect.
    Boehm.

  Non-temporal stores:
    For memcpy, using temporal stores cuts throughput by 1/3. This is for
    linear memory which won't be read again until the cache has been flushed,
    our GC environment is different. We copy a graph and we read each object
    twice, once when evacuating, once when scavenging. Maybe it's only worth
    it to use a non-temporal store when we have evacuated many more objects
    than we have scavenged. That is, when the to-pointer is far from the
    scavenge-pointer. In this case, by-passing the cache might be best.

Whole program alias analysis:
  Alias analysis is import for allowing LLVM to optimize properly. In
  particular, LLVM cannot move invariant loads out of loops if indeed
  it doesn't know which loads are invariant.
  GHC saw a decent performance improvement (average of about 4% on nofib)
  when they enabled basic aliasing analysis. The improvement came nearly
  entirely from hoisting loads out of strict loops.
  The HPT analysis we do on bedrock code should give us much better
  aliasing data than what GHC can muster. Not only can tell LLVM about
  trivial things (Stack pointers and heap pointers don't alias, heap
  isn't directy modified by other programs, etc); We can tell LLVM exactly
  which heap pointers alias each other and which do not.
  This should be important for performance so make sure to benchmark it to
  verify. If no performance improvements are seen, figure out why.

Interprocedural register allocation:
  LLVM is going a great job at this. But how does it scale? For small programs,
  LLVM does whole-program interprocedural register allocation. What will it do
  for large programs? Maybe we need to help it along by grouping SCCs. Maybe
  LLVM does this on its own.

Things to benchmark:
  How much does it cost to limit the mutator with a fuel argument?
  How does immix compare to an ordinary copying collector?
  What percentage of of thunks are forced? A high percentage would
  mean eager evaluation is beneficial.
  How much of the heap is wasted on pointers? A space optimal heap would
  only have pointers to shared objects. Knowing how far we are from the
  optimal would give us the limit of gains possible by merging nodes.

  When garbage collecting a generation, how many pointers are there on
  average to the next generation? GHC will read from these pointers but
  avoid them. How significant is this?

  Haskell, LLVM, and Alias Analysis
    Blog post. Benchmark the impact of interprocedural alias analysis.
  Haskell, LLVM, and Register Allocation
    Blog post. Benchmark the impact of interprocedural register allocation.
  Haskell, GC, and Compaction
    Blog post. Benchmark the impact of compacting live objects.
  Haskell, GC, and Non-Temporal Stores
    Blog post. Benchmark the impack of non-temporal stores in the GC.
  Haskell, C, and Callbacks
    Blog post. Benchmark the cost of calling Haskell from C.

Stepping in generational garbage collectors:
  Problem: Not all objects in a generation will be given the same amount of
  time to expire. If we promoted (moved to the next generation) all live
  objects on each GC run then objects recently allocated would be promoted
  too early.

  Solution 1: Block-wise promotion.
  Say our generation contains 10 numbered blocks. Now, when collecting, we can
  selectively promote only those objects where were allocated in the first
  five blocks. All other objects will be placed back into the current
  generation, to be promoted in the next GC run.
  GHC uses this solution.

  Solution 2: Per-object stepping counter.
  Each object notes down how many times it has survived a collection. Once it
  has survived enough, it'll be promoted.

Exceptions: One shadow stack vs. two shadow stacks:
  One shadow stack:
    Fewer registers used.
    More expensive throws.
    Returning across a catch frame doesn't change.

Targeting LLVM for a functional language:
  We need low-overhead allocation, tail calls, accurate GC, green threads, and
  exception handling. LLVM tries to provide support for garbage collection and
  exceptions but it falls woefully short of anything useful. Instead, LHC
  solves all five issues by allocating stack frame on the heap and managing
  them manually.

  Low-overhead allocation. Allocation is usually done by bumping a pointer
  until it reaches some limit. Keeping this heap pointer and the heap limit
  in registers has been shown to be significant for performance. Global
  variables /can/ be used but nether GCC nor LLVM will try very hard at
  keeping them in registers. A better approach is to pass them along as
  arguments to all Haskell functions.

  Tail calls. Manually managing the stack passes the burden of doing
  tail-call optimization on to our compiler. Fortunately, this is a trivial
  burden and we're glad to take on the responsibility.

  Accurate GC. Finding all root pointers requires knowledge of the stack
  layout. Even with knowledge of the stack layout, most compilers require
  the stack to only contain pointers and not entire objects. In LHC, since
  the stack is completely exposed, we can be much more aggressive about placing
  objects on the stack. However, since stack frames are allocated on the heap,
  the advantages are not huge. At the very least we'll minimize the number of
  small allocations by grouping them together in the stack frame. And in a few
  cases, notably when a function will neither throw exceptions nor call the
  scheduler, we are able to allocate the stack frame on the system stack. LLVM
  can then do register allocation freely.

  Green threads. Green threads are suspended functions that are scheduled by
  the RTS (usually written in user-space), not the OS. Suspending and resuming
  is trivial when we're managing the stack manually. Additionally, green
  threads go hand-in-hand with event driven IO.

  Exception handling. Like green threads, exception handling is trivial with
  an explicit stack. Just unroll the frames until an exception handling frame
  is found.


  I believe the move away from a linear stack is the principled and correct
  approach for LHC. A linear stack is problematic for garbage collection and
  exception handling, and downright wrong for green threads (having 1,000,000
  stacks for 1,000,000 suspended functions is a drag on performance, especially
  since the stacks are only used during execution and only a few of the
  threads are ever executed at once). Having one stack per execution core and
  the context of a suspended function as a linked list seems more appropriate.
  Moving allocations from the heap onto the system stack is then an
  optimization like any other.

Foreign Function Interface:
  unsafe calls
    Trivial, direct ccalls.
  safe calls
    Spawned in a work queue (libuv). Result passed back into Haskell space
    via libnanomsg.
  C -> Haskell
    Haskell functions aren't called directly. Instead requests are passed
    into Haskell space via libnanomsg.
  wrapper: IO () -> IO (FunPtr (IO ()))
  dynamic: FunPtr (IO ()) -> IO ()
    safe call created dynamically. No option for unsafe calls?

Libraries to make life easier:
  libuv
    Manages the event loop per execution core.
  libjudy
    Provides efficient StablePtr -> Closure mapping.
    Efficient remembered set?
  libnanomsg
    Responsible for all communication between execution cores and between
    C space and Haskell space (eg. calling Haskell from C).
  tomfastmath
    Efficient arbitrary precision integer implementation.
  lifffi
    Can generate function pointers on the fly. This is needed for, say, foreign
    wrappers.

GRIN optimizations:
  Copy propagation:
    This is done by LLVM.
  Generalised unboxing:
    This is done by whole-program dead variable elimination. LLVM might be
    able to do this in many cases but we better make sure it gets done
    correctly.
  Evaluated case elimination:
    Eliminate senseless code such as:
    case scrut of
       P1 -> unit scrut
       P2 -> unit scrut
    LLVM does this.
  Trivial case elimination:
    case scrut of
      P1 -> ...
    LLVM deals with this.
  Sparse case optimisation:
    Remove unused case branches. LLVM cannot do this. It relies on the HPT
    analysis.
  Case copy propagation:
    v <- case scrut of
      P1 -> node (Int x)
      P2 -> node (Int y)
    n <- case scrut of
      P1 -> x
      P2 -> y
    v = node (Int n)
    Done by LLVM? 
  Update elimination:
    Not done by LLVM. Requires HPT analysis results.
  Late inlining:
    Done by LLVM.
  Case hoisting:
    Done by LLVM.
  Constant propagation:
    Done by LLVM.
  Arity raising.
    Done partly by LLVM. Also has to be done in Bedrock.
  Common sub-expression elimination:
    Done by LLVM.
  Dead code elimination:
    Partly done by LLVM. Whole-program dce done in bedrock.
  Dead parameter elimination:
    Same as above.

Graph compression:
  Remove tail pointers. Always a win.
  Eliminate pointers to static objects (Nil, Empty, etc). Combinatorial
  explosion.
  Inlining nodes. Gets in the way of tail pointer elimination.
  Unpacking nodes. Destroys sharing.
  
  If we annotated the graph with sharing information, destructive optimizations
  would become safe. When moving an object, we replace it with an Indirection.
  When we find a pointer to an Indirection, we set the shared bit for the
  indirection target and replace the Indirection with IndirectionShared. When
  we encounter an IndirectionShared, we do nothing (like we did with an
  Indirection before). This will cost an extra 2 loads and stores for each
  shared object. This information will quickly be out of date. And unshared
  objects can become shared in the future. Hm, doesn't seem worth it.

  Node inlining requires speculative fetching during evacuation. Hm, if we
  tagged pointers with node types then this fetch could be avoived. But we
  also want to tag pointers with the generation they're in. This generation
  tag is read when updating and in the GC loop so it's probably more important.
  Hm, we don't need to know which node we're pointing to. We just need to know
  whether it can be inlined. A single bit is required. On 32bit machines there
  aren't enough bits. But on 64bit machines, we can do it. One bit for
  inlineable, two bits for generation.

  Since we have to update with redirections, nodes cannot be smaller than two
  words. This means we cannot inline all children. For example,
  (Cons (Char _) Nil) cannot be packed without a pointer to the Char. Static
  nodes such as Nil or Nothing are exempt, though. For those objects, we do not
  care about duplication.


GHC GC Notes:
  Evacuate: (p is the address of a root pointer)
    1. q = *p;
    2. Check if p is HEAP_ALLOCED
      ezyang is working on making this cheaper.
    3. Read block descr for q
    4. Read info table for q
    5. Find destination gen
    6. to = gen->free
    7. gen->free += size
    8. if gen->free > gen->limit todo_block_full
    9. copy from -> to
    10. update q with indirection
    11. *p = to

  Can we avoid the stack? How many registers do we need? Reading from the cache
  is cheap, writing is always expensive. Put destination pointers in cache,
  limits in mem?

Pointer and node tagging:
  Heap pointers are tagged with inlineable:1 and gen:2. On 32bit systems,
  inlineable is skipped as assumed to be always false.
  Hm, for updates, have pointers tagged with gen_no shouldn't be a big win.
  Since we're overwriting the tag, reading the tag should be nearly free.
  Tagging gen_no should make the GC slightly faster since it doesn't have to
  touch nodes from other generations than it is currently GCing. Maybe this is
  not very significant.

  Nodes are of course tagged with their ID number. They're tagged with
  a stepping counter, 3 or 4 bits should be enough, maybe 2. They're tagged with
  a mark bit, only used by the mark/sweep collector. And importantly, they're
  tagged with a bitmap of which pointers are inlined.
  18 bits for the ID number. Three bits for the stepper. One for the mark. This
  leaves 32-22=10 bits for the bitmap on 32bit machines. 64-22=42 bits on 64bit
  machines. This should definitely cover more than 99.9% of objects.

GC strategies and memory overhead:
  Four generations, last generation being immix.

  Single walk-through strategy:
    For each of the semi-space generations, set aside memory for copying. This
    doubles the worst-case memory bound.
    At the beginning of each GC run, we determine how many generations we need
    to collect. Collecting an older generation also collects the younger
    generations.
    Imagine we collect generation 0 and at the end of the collection, generation
    1 has exceeded its limit. This does not immediately trigger a new
    collection. Instead, generation 1 is marked for collection at the next run
    and generation 0 is shrunk if necessary to keep the worst-case memory
    bound within its limit. At the next GC run, both generation 1 and 0 are
    collected together.
    This approach can collect several generations in a single pass-through but
    requires a lot of memory.
    Worst case memory bound: size of all generations * 2

  Gen-by-gen strategy:
    In this strategy we free the from-space of each generation before
    collecting the next. This requires more passes over the heap but reduces
    the worst case memory bound.
    We take the larget generation and set aside enough memory to collect it.
    Now imagine that we collect generation 0 and at the end of the collection,
    generation 1 has exceeded its limit. We free the from-space for generation 0
    and walk the to-space to find additional roots (all the roots we used for
    collecting generation 0 still counts as roots). If, at the end of this
    collection, genration 2 has exceeded its limit, we do the same as we did
    before: Free the from-space of generation 1 and walk generation 0+1 to
    find additional roots.
    Worst case memory bound: size of the largest generation * 2

  META NOTE: Rename 'factor' to 'ratio'. It describes the ratio of live objects
             to allocation space.
  Per generation expansion factor:
    The size of each generation is determined as the size of the live objects
    multiplied by some factor+1.
    If, for the sake of simplicity, we're using the single walk-through
    strategy, a factor of 1 would mean size=live*(1+1) and memory_bound=size*2,
    therefore memory_bound=live*4. In the worst case, we'll use 4 times the
    size of our live objects. However, the steady-state case would be just
    steady_state=live*(factor+1)+live=live*(factor+1+1)=live*3. In the common
    case, we only use 3x the size of the live objects.

    But the factors don't have to be the same for each generation. We might
    want to make allocations cheaper in generation 0 at the cost of increased
    memory usage. And we might want to make generation 2 more space efficient
    at the cost of a higher rate of GCing.
    Generation 0, 1 and 2 might have factors of, say, 2, 1 and 0.5,
    respectively.

    The gen-by-gen strategy works best if the generations are roughly evenly
    sized. The relevant parameters here are the size factor and the stepping
    count.

  When we get close to the memory limit:
    When we exceed the memory limit, we don't have enough space to expand
    the generations by as much as they would like. When this happens, we scale
    back the factors uniformly. If we use the factors 1,1,1 then we might
    scale them to 0.5,0,5,0,5. If we use the factors 2,1,0.5 then we might
    scale them to 1,0.5,0.25.

    This has the effect of triggering GCs more often. The percentage of time
    spent in GC goes up. Object are promoted quicker to the immix generation.
    Once they're in the immix generation, space usage goes down and the
    semi-space collectors may be free to expand again.

  Memory overhead:
    The overhead for immix is 25% for marking and X% for compaction. 12% may
    be more than enough to set aside for compaction. That gives a total of 37%
    overhead for objects in immix (assuming no loss due to fragmentation). We
    don't actually pay for this overhead in the common case. But we need to
    calculate the worst-case memory usage when adjusting to the maximum heap
    size.

    Overhead for a factor of 0 is 100%.
    Overhead for a factor of 0.5 is 150%.
    Overhead for a factor of 1 is 200%.
    Overhead for a factor of 2 is 300%.

    Large objects are not moved and therefore have zero overhead (except for
    fragmentation loss when not a multiple of pagesize).

Block allocator and the GC:
  Global list of free blocks.
  Large objects are allocated separately and never moved.
  An arena is a list of used blocks and used large objects.
  No coalescing. Block size relative to the size of the heap. Small blocks
  are resized (Linux only) or freed. Large blocks are broken into smaller
  blocks.
  Allocating a new continuous slice of memory from an arena either
  grabs a block from the global list or allocates a new block with mmap.
  An arena is freed by moving the used blocks to the global block list.


  data Arena = Arena LinkedListOfBlocks LinkedListOfLargeObjects
  newSlice :: Arena -> IO (Ptr, Size)
  free :: Arena -> IO ()
  allocateLarge :: Arena -> Size -> IO Ptr
