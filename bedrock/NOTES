Inefficiencies in GHC:
  Objects are placed on the heap instead of the stack.
  Each green thread has its own stack.
  No stepping in the GC.
  GCing CAFs.

Interesting avenues to explore:
  Immix.
  Supercompilation.
  Eager evaluation.

Profiling, debugging and program monitoring:
  Output dianostics continuously via nanomsg.
  Memory snapshot after each GC:
    Per generations:
      Per object type:
        Number of live
        Total size used
      Size of live objects
      Total size of generation
      Number of pointers to the previous generation
  Object duration is GC interval * (stepping+0.5)
  External tools can then listen to the snapshots and generate a graph of
  memory usage over time.

Shared nothing concurrency.
  Allocate a fixed number of isolated heaps per core.
  Round-robin allocation of new actors in each core/heap pair.
  Communication primitives:
    IORef: Simple mutable referrences. no locks, no sharing, no magic.
    MVar: Like IORefs but can block on reads and writes. The scheduling is part
          of the user-level implementation, not the RTS. No locks, no sharing.
    SVar: Like MVar but can be used to share values between actors. Has higher
          overhead than an MVar.
    SChan: Like Chan but can be shared.

Holes
  How to compile this: let ones = 1 : ones

  ptr <- store (Cons 1 hole)
  update ptr[2] ptr

  ptr <- alloc
  update ptr (Cons 1 ptr)

  Implicit fix-point? Lowered to alloc+update later.
  ptr <- store (Cons 1 ptr)

Garbage collection of CAFs.
  Trivial. We statically know which CAFs are referrable by all
  suspended functions.

Constant objects.
  GHC statically allocates a bag of small objects (ints and chars of small
  value) because they're very common. Would it be worth it to generalize this
  technique?

Garbage collection:
  Copying collector:
    Evacuate roots.
    Scavenge to-space.

  Mark/Sweep:
    How is marking done without blowing the stack? It isn't. :(
    The mark queue could be allocated on the heap. The smallest chain link
    take up 2 words. Since the mark queue needs roughly 1 word per chain link,
    we'd need to reserve 1/3 of the heap for marking.
    By marking tail-recursively we can queue one child fewer. Thus making the
    smallest chain link take up 3 words. Reserved space would then be 1/4 of
    heap. If we use another 1/4 to allow for compaction then only half the
    heap is available to the mutator before a GC is triggered.

  Trivial collectors:
    NoGC, fixed. Allocate a single chunk, never grow it.
    NoGC, growing. Allocate as needed, never collect.
    Boehm.

Whole program alias analysis:
  Alias analysis is import for allowing LLVM to optimize properly. In
  particular, LLVM cannot move invariant loads out of loops if indeed
  it doesn't know which loads are invariant.
  GHC saw a decent performance improvement (average of about 4% on nofib)
  when they enabled basic aliasing analysis. The improvement came nearly
  entirely from hoisting loads out of strict loops.
  The HPT analysis we do on bedrock code should give us much better
  aliasing data than what GHC can muster. Not only can tell LLVM about
  trivial things (Stack pointers and heap pointers don't alias, heap
  isn't directy modified by other programs, etc); We can tell LLVM exactly
  which heap pointers alias each other and which do not.
  This should be important for performance so make sure to benchmark it to
  verify. If no performance improvements are seen, figure out why.

Interprocedural register allocation:

Things to benchmark:
  How much does it cost to limit the mutator with a fuel argument?
  How does immix compare to an ordinary copying collector?
  What percentage of of thunks are forced? A high percentage would
  mean eager evaluation is beneficial.

Stepping in generational garbage collectors:
  Problem: Not all objects in a generation will be given the same amount of
  time to expire. If we promoted (moved to the next generation) all live
  objects on each GC run then objects recently allocated would be promoted
  too early.

  Solution 1: Block-wise promotion.
  Say our generation contains 10 numbered blocks. Now, when collecting, we can
  selectively promote only those objects where were allocated in the first
  five blocks. All other objects will be placed back into the current
  generation, to be promoted in the next GC run.
  GHC uses this solution.

  Solution 2: Per-object stepping counter.
  Each object notes down how many times it has survived a collection. Once it
  has survived enough, it'll be promoted.

Targeting LLVM for a functional language:
  We need low-overhead allocation, tail calls, accurate GC, green threads, and
  exception handling. LLVM tries to provide support for garbage collection and
  exceptions but it falls woefully short of anything useful. Instead, LHC
  solves all five issues by allocating stack frame on the heap and managing
  them manually.

  Low-overhead allocation. Allocation is usually done by bumping a pointer
  until it reaches some limit. Keeping this heap pointer and the heap limit
  in registers has been shown to be significant for performance. Global
  variables /can/ be used but nether GCC nor LLVM will try very hard at
  keeping them in registers. A better approach is to pass them along as
  arguments to all Haskell functions.

  Tail calls. Manually managing the stack passes the burden of doing
  tail-call optimization on to our compiler. Fortunately, this is a trivial
  burden and we're glad to take on the responsibility.

  Accurate GC. Finding all root pointers requires knowledge of the stack
  layout. Even with knowledge of the stack layout, most compilers require
  the stack to only contain pointers and not entire objects. In LHC, since
  the stack is completely exposed, we can be much more aggressive about placing
  objects on the stack. However, since stack frames are allocated on the heap,
  the advantages are not huge. At the very least we'll minimize the number of
  small allocations by grouping them together in the stack frame. And in a few
  cases, notably when a function will neither throw exceptions nor call the
  scheduler, we are able to allocate the stack frame on the system stack. LLVM
  can then do register allocation freely.

  Green threads. Green threads are suspended functions that are scheduled by
  the RTS (usually written in user-space), not the OS. Suspending and resuming
  is trivial when we're managing the stack manually. Additionally, green
  threads go hand-in-hand with event driven IO.

  Exception handling. Like green threads, exception handling is trivial with
  an explicit stack. Just unroll the frames until an exception handling frame
  is found.


  I believe the move away from a linear stack is the principled and correct
  approach for LHC. A linear stack is problematic for garbage collection and
  exception handling, and downright wrong for green threads (having 1,000,000
  stacks for 1,000,000 suspended functions is a drag on performance, especially
  since the stacks are only used during execution and only a few of the
  threads are ever executed at once). Having one stack per execution core and
  the context of a suspended function as a linked list seems more appropriate.
  Moving allocations from the heap onto the system stack is then an
  optimization like any other.



